{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0f4eba",
   "metadata": {},
   "source": [
    "# Simple chatbot using Hugging Face Transformers library! \n",
    "\n",
    "Try it talking to a chatbot through the GUI!\n",
    "\n",
    "**How it works:**\n",
    "1. Run the cell and wait for the tkinter window to pop out\n",
    "2. Start chatting!\n",
    "\n",
    "**Required modules:**\n",
    "- `pip install transformers`\n",
    "- `pip install torch`\n",
    "\n",
    "**Optional modules:**\n",
    "- `pip install huggingface_hub[hf_xet]` for better model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c599442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366d623152b04ab49e2d55ac9f11452b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\sideprojectenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gerra\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69bf7de978340c983cb83bc014c3e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c378b578194b70892a4e909d8b2354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff215417b105456595ca3ab0de10ebcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e140a5867ac84e56a1bca10066aa6831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2b5dccd71249eca1a7d8749aca2fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900529ce765948f0ba19474bc13cee59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import Entry, scrolledtext, Button, Label\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# nicer GUI + conversation support (multiple messages)\n",
    "window = tk.Tk()\n",
    "window.title(\"WebIdeasBot\")\n",
    "window.geometry(\"800x520\")\n",
    "\n",
    "chat_window = scrolledtext.ScrolledText(window, width=95, height=24, state=tk.DISABLED, wrap=tk.WORD)\n",
    "chat_window.configure(font=(\"Consolas\", 11))\n",
    "chat_window.pack(padx=8, pady=(8, 4), fill=tk.BOTH, expand=False)\n",
    "\n",
    "controls_frame = tk.Frame(window)\n",
    "controls_frame.pack(fill=tk.X, padx=8, pady=(0,8))\n",
    "\n",
    "user_input = Entry(controls_frame, width=72, font=(\"Consolas\", 11))\n",
    "user_input.grid(row=0, column=0, padx=(0,8), sticky=\"we\")\n",
    "user_input.focus_set()\n",
    "\n",
    "send_btn = Button(controls_frame, text=\"Send\", width=10)\n",
    "send_btn.grid(row=0, column=1, padx=(0,8))\n",
    "clear_btn = Button(controls_frame, text=\"Clear\", width=10)\n",
    "clear_btn.grid(row=0, column=2)\n",
    "\n",
    "status_label = Label(window, text=\"Loading model... (this may take a while)\", anchor=\"w\")\n",
    "status_label.pack(fill=tk.X, padx=8)\n",
    "\n",
    "tokenizer = None\n",
    "model = None\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "conversation_history = []  # list of tuples: (\"User\"/\"Bot\", text)\n",
    "lock = threading.Lock()\n",
    "\n",
    "def append_chat(role, text):\n",
    "    chat_window.configure(state=tk.NORMAL)\n",
    "    chat_window.insert(tk.END, f\"{role}: {text}\\n\\n\")\n",
    "    chat_window.see(tk.END)\n",
    "    chat_window.configure(state=tk.DISABLED)\n",
    "\n",
    "def load_model():\n",
    "    global tokenizer, model\n",
    "    try:\n",
    "        # Using DialoGPT-medium for better quality responses\n",
    "        model_name = \"microsoft/DialoGPT-medium\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        # Add the EOS token as PAD token to avoid warnings\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        window.after(0, lambda: _on_model_ready())\n",
    "    except Exception as e:\n",
    "        window.after(0, lambda: status_label.config(text=f\"Model load error: {e}\"))\n",
    "\n",
    "def _on_model_ready():\n",
    "    status_label.config(text=f\"Model loaded. Device: {device.type}\")\n",
    "    append_chat(\"System\", \"Model loaded. You can start chatting.\")\n",
    "\n",
    "threading.Thread(target=load_model, daemon=True).start()\n",
    "\n",
    "def _is_bad_response(s: str) -> bool:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return True\n",
    "    # too short\n",
    "    if len(s) < 2:\n",
    "        return True\n",
    "    # only punctuation / underscores / newlines\n",
    "    if set(s) <= set(\" _-.\\n\"):\n",
    "        return True\n",
    "    # same token repeated many times\n",
    "    tokens = [t for t in re.split(r\"\\s+\", s) if t]\n",
    "    if tokens and len(set(tokens)) == 1 and len(tokens) > 3:\n",
    "        return True\n",
    "    # check for gibberish (high ratio of consonants to vowels)\n",
    "    if len(s) > 10:\n",
    "        vowels = sum(1 for c in s.lower() if c in 'aeiou')\n",
    "        consonants = sum(1 for c in s.lower() if c.isalpha() and c not in 'aeiou')\n",
    "        if vowels > 0 and consonants / vowels > 4:  # more than 4 consonants per vowel\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _clean_response(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # remove leading/trailing whitespace and repeated identical lines\n",
    "    lines = [ln.rstrip() for ln in s.splitlines() if ln.strip()]\n",
    "    cleaned = []\n",
    "    prev = None\n",
    "    for ln in lines:\n",
    "        if ln == prev:\n",
    "            continue\n",
    "        cleaned.append(ln)\n",
    "        prev = ln\n",
    "    # join and trim runaway repeats\n",
    "    out = \"\\n\".join(cleaned).strip()\n",
    "    # Remove any unwanted prefixes that the model might generate\n",
    "    out = re.sub(r\"^(User|Human|Bot):\\s*\", \"\", out)\n",
    "    # Remove any URLs or gibberish strings\n",
    "    out = re.sub(r'http\\S+|www\\.\\S+|\\S*[0-9][0-9]\\S*', '', out)\n",
    "    # Remove multiple spaces\n",
    "    out = re.sub(r'\\s+', ' ', out)\n",
    "    return out.strip()\n",
    "\n",
    "def generate_response_sync(prompt_text):\n",
    "    if tokenizer is None or model is None:\n",
    "        return \"[Model still loading]\"\n",
    "    \n",
    "    try:\n",
    "        # Encode the input\n",
    "        inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        attention_mask = torch.ones(inputs.shape, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Generate response with carefully tuned parameters\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,  # Keep responses concise\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,  # More focused responses\n",
    "            top_p=0.9,  # Nucleus sampling\n",
    "            top_k=40,  # Limit vocabulary diversity\n",
    "            repetition_penalty=1.3,  # Prevent repetition\n",
    "            no_repeat_ngram_size=3,  # Prevent repetition of phrases\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode only the new tokens\n",
    "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        response = _clean_response(response)\n",
    "        \n",
    "        if not _is_bad_response(response):\n",
    "            return response\n",
    "        \n",
    "        # If first attempt failed, try with different parameters\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,  # Even more focused\n",
    "            top_p=0.85,\n",
    "            top_k=30,\n",
    "            repetition_penalty=1.4,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        response = _clean_response(response)\n",
    "        \n",
    "        if not _is_bad_response(response):\n",
    "            return response\n",
    "            \n",
    "        return \"I'm not sure how to respond to that. Could you try rephrasing your message?\"\n",
    "    except Exception as e:\n",
    "        return f\"[Error generating response: {e}]\"\n",
    "\n",
    "def generate_response_async():\n",
    "    with lock:\n",
    "        status_label.config(text=\"Generating...\")\n",
    "        send_btn.config(state=tk.DISABLED)\n",
    "        user_input.config(state=tk.DISABLED)\n",
    "\n",
    "    # Format conversation history for the model\n",
    "    max_turns = 4  # Reduced from 6 to keep context more focused\n",
    "    recent = conversation_history[-max_turns*2:]\n",
    "    prompt_parts = []\n",
    "    for role, msg in recent:\n",
    "        if role == \"User\":\n",
    "            prompt_parts.append(msg)\n",
    "    prompt_text = \" \".join(prompt_parts[-2:])  # Only use last 2 user messages for context\n",
    "\n",
    "    response = generate_response_sync(prompt_text)\n",
    "\n",
    "    # update UI back on main thread\n",
    "    def finish():\n",
    "        with lock:\n",
    "            append_chat(\"WebIdeasBot\", response)\n",
    "            conversation_history.append((\"Bot\", response))\n",
    "            status_label.config(text=\"Ready\")\n",
    "            send_btn.config(state=tk.NORMAL)\n",
    "            user_input.config(state=tk.NORMAL)\n",
    "            user_input.focus_set()\n",
    "    window.after(0, finish)\n",
    "\n",
    "def on_send(event=None):\n",
    "    text = user_input.get().strip()\n",
    "    if not text:\n",
    "        return\n",
    "    append_chat(\"You\", text)\n",
    "    conversation_history.append((\"User\", text))\n",
    "    user_input.delete(0, tk.END)\n",
    "    # start generation in background\n",
    "    threading.Thread(target=generate_response_async, daemon=True).start()\n",
    "\n",
    "def on_clear():\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    chat_window.configure(state=tk.NORMAL)\n",
    "    chat_window.delete(\"1.0\", tk.END)\n",
    "    chat_window.configure(state=tk.DISABLED)\n",
    "    status_label.config(text=\"Cleared conversation. Model ready.\" if model is not None else \"Cleared conversation.\")\n",
    "\n",
    "send_btn.config(command=on_send)\n",
    "clear_btn.config(command=on_clear)\n",
    "user_input.bind(\"<Return>\", on_send)\n",
    "\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sideprojectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
